= Automating with pipelines

Red Hat OpenShift AI gives the possibilities to create pipelines by dragging and connecting Notebooks using the Elyra extension. The second alternative, -and the one we will use- is by importing an existing pipeline in YAML format created using the KubeFlow Pipelines SDK. This programmatic platform uses Python to simplify the process of writing and building the pipeline.

== Import Pipeline

Once the server is set up, we will create a pipeline to fully automate the process. This pipeline will collect new data from the BMS application and use it to retrain a new model. The performance of this new model will then be compared against the existing one. If it outperforms the current model, it will be uploaded to the S3 bucket, where Model Serving will automatically deploy it. This process will be repeated for both the Stress Detection and Time to Failure Prediction models.

In order to save some time we will import it instead of creating it manually. If you want to learn more about how to create pipelines, check the https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.18/html/openshift_ai_tutorial_-_fraud_detection_example/implementing-pipelines#create_a_pipeline[documentation].

. First, open the *Jupyter Environment* with the Notebooks used to train the models and test the endpoints. 
. In the folder structure on the left, locate the `pipelines` folder and open it. 
. Locate the `model_retraining.yaml` pipeline file. *Right-click* on it and select *Download*.
. Save the file locally in your laptop.

image::5-2_pipeline-download.png[Download pipeline file]

[start=5]

. Now, go back to your *Red Hat OpenShift AI Dashboard*.
. Make sure you are in the *Pipelines* tab inside our project.
. Select *Import pipeline* and complete the form as follows:
 ** *Project*: The `ai-edge-project` project should be selected by default.
 ** *Pipeline name*: Name it as `Model retraining`.
 ** Verify that the *Upload a file* check is marked.
 ** Click the *Upload* button and select the `model_retraining.yaml` from your laptop.
. When completed, press *Import pipeline*.

You should see the different pipeline nodes and steps displayed in a graphical way, like this:

image::6-2_pipeline-imported.png[Imported Pipeline]

== Schedule recurring execution

Our machine learning models need to adapt to evolving data patterns to maintain their accuracy and relevance. By scheduling our pipeline to run automatically every 10 minutes, we ensure that both the Stress Detection and Time to Failure Prediction models are continuously retrained with the latest battery data collected from the BMS application. This automated approach eliminates manual intervention, reduces the risk of model drift, and guarantees that our predictive models always reflect the most current battery behavior patterns.

Now let's configure the pipeline to execute automatically:

. Once the pipeline is imported, click on the blue *Actions* button in the top-right corner. 
. Select *Create schedule* from the dropdown menu.
. Fill in the schedule configuration form:
 ** *Name*: `Scheduled run`
 ** *Trigger type*: Select *Periodic*
 ** *Run every*: `10` `minutes`
. Scroll down and verify that the *Model Retraining* pipeline has been automatically selected.
. Check that the *Parameters* match the following values:
+
[cols="1,1", subs="attributes+"]
|===
|Parameter |Value

|aws_access_key_id
|minio

|aws_s3_bucket
|s3-storage

|aws_s3_endpoint
|http://minio-service.minio.svc.cluster.local:9000

|aws_secret_access_key
|minio123

|influxdb_bucket
|bms

|influxdb_org
|redhat

|influxdb_token
|admin_token

|influxdb_url
|https://influx-db-microshift-001.{openshift_cluster_ingress_domain}/
|===

. When all parameters are correctly configured, click *Create schedule*.

The pipeline will now execute automatically every 10 minutes, continuously improving your models with fresh battery data. Come back after 10 minutes and the first scheduled run should be underway. Once it finishes, all the steps should have a green tick like the one shown below:

image::6-2_pipeline-complete.png[Pipeline finished execution]





