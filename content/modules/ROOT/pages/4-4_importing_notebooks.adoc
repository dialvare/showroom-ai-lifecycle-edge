= Importing and navigating through Notebooks

== Clone the repository

Usually, model development and training is the most time-consuming part. On JupiterLab we can use Notebooks to develp and run the code needed to generate our AI models. In this demo, instead of creating the Notebooks and the code on our own, we are going to clone this link:https://github.com/dialvare/ai-lifecycle-edge.git[GitHub repository] that contains the Notebooks and adititonal resources that we will need later.

. Make sure you are in your workbench's JupyterLab platform. There's a toolbar on the left. There, in the third position you will find the *Git* icon. Click it.


. In the new menu, select the *Clone repository* option. You will be asked to type the URL to the repo. Copy and paste this one:
+
[.console-input]
[source,sh]
----
https://github.com/dialvare/ai-lifecycle-edge.git
----

[start=3]

. Click *Clone* to start pulling the contents.

You can navigate through the different folders on the left section.

== Inspect the Notebooks

Now that you have all the data needed, navigate to the *notebooks* folder. All of them are ordered as shown below:

* *0.Transform_dataset*: Creates the raw datasets and uploads them to your s3 bucket.
* *1.Collect_data*: Pulls and modifies the raw datasets to make them available for training.
* *2.Re-train_model*: Trains and validates the model using historical and new data.
* *3.Save_model*: Uploads the onnx forecast model to the s3 bucket.
* *4.Upload_data*: Uploads the updated datasets with the data already used.

*Do not run them yet!*. As you can see, some of the notebooks will pull and store data from/to the MinIO bucket. To be able to access the service, we first need to attach a data connection to our workbench.

== Next section:

In the following section we will see how to create the data connection and to use it to store our models.
