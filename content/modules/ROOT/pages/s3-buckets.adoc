= Creating the s3 MinIO buckets

== Model bucket

When we train an AI/ML model, a crutial step is to make the data accessible during the training process. Also, once we obtain the new model we need a place where to store it so it can be served. Therefore, some kind of storage is needed. We have seen that our node comes with MinIO deployed, so the next thing will be creating a bucket. Buckets are similar to folders in a filesystem and are used to organize objects.

. First, we need to access the MinIO Console. Open a new tab in your browser and access the following URL:
+
[.console-input]
[source,sh]
----
https://minio-ui-ai-edge-project.apps.cluster-79242.dynamic.redhatworkshops.io
----

[start=2]

. You will be directed to the logging page. Use the credentials provided below to access it:
 ** *Username*: `minio`
 ** *Password*: `minio123`
. In the Object Browser landing page you will not find any buckets yet, so click *Create a Bucket* and complete the *Bucket name* with `s3-storage`.
. Finally, press the *Create Bucket* button.

Cool! Our fist bucket is created. We will use it later to store the model data.

== Pipelines bucket

The second bucket we will need is the one that will store all the artifacts generated when creating and running pipelines in Red Hat OpenShift AI. Let's do again the same process we already followed.

. Navigate back to the *Buckets* section on the left menu. Now you should see the one we created before.
. In the apper-right corner, click on *Create Bucket +*. This time, the *Bucket name* will be `pipelines`.
. And then, select _Create Bucket_.

And just like that we have finished configuring our s3 storage. Now the Buckets should look like the following one:

[IMAGE_HERE]

== Next section:

At this point, we are ready to start using the s3 storage to automate the lifecycle. As we commented before, the central piece in charge of all the components involved will be Red Hat OpenShift AI. It's time to install the operator!
